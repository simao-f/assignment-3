{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y5fgLs4b_kII"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T3_VJYy_zry",
        "outputId": "00dfe247-22c3-4c2e-d16e-d2a817ae45ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/284.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/284.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf --q\n",
        "!pip install -qqq chromadb==0.4.10 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off!pip install -Uqqq pip --progress-bar off\n",
        "# !pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.299 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.21 --progress-bar off\n",
        "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
        "!pip install -qqq tokenizers==0.14.0 --progress-bar off\n",
        "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off\n",
        "!pip install -qqq unstructured==0.10.16 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZPCUzOs_2RR",
        "outputId": "ad39c945-2c60-44d7-813b-906a77f8d7af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "loader = PyPDFLoader(\"CV_data.pdf\")\n",
        "\n",
        "docs = loader.load()\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjosSN-OJjqY"
      },
      "source": [
        "The Markdown file we're loading is the CV data paper. Let's see how we can use the RecursiveCharacterTextSplitter to split the document into smaller chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRyMG_Oj_-er",
        "outputId": "a028113d-9bd0-493d-e63a-27cd290dd501"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "82"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwUss3ipJzGc"
      },
      "source": [
        "Splitting the document into chunks is required due to the limited number of tokens a LLM can look at once (4096 for Llama 2). Next, we'll use the HuggingFaceEmbeddings class to create embeddings for the chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz3zZVX8JwH7",
        "outputId": "ffe72937-aa36-4738-9f97-5972f9eefa94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1024\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"thenlper/gte-large\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "print(len(query_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq1Dm5UoNSah"
      },
      "source": [
        "We'll use Chroma database to store/cache the embeddings and make it easy to search them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoBIoD8GNRq1",
        "outputId": "14d8bb49-cfcf-4bfe-a695-dc7be2986d7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "articles, book chapters, book reviews, and select commentary. After adding your publications, you need to focus on conferences and workshops.\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n",
        "results = db.similarity_search(\"Transformer models\", k=2)\n",
        "print(results[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8l9qoCR8UdyB"
      },
      "outputs": [],
      "source": [
        "# Assuming there's a configuration object or function that accepts backend settings\n",
        "config = {\n",
        "    # Other configuration settings\n",
        "    \"disable_exllama\": True\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Sd5YYnPqRrp7",
        "outputId": "39342b84-d536-4725-e331-bb47e7322dce"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "MODEL_NAME = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"cuda\"\n",
        ")\n",
        "\n",
        "# Create a configuration for text generation based on the specified model name\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set the maximum number of new tokens in the generated text to 1024.\n",
        "# This limits the length of the generated output to 1024 tokens.\n",
        "generation_config.max_new_tokens = 1024\n",
        "\n",
        "# Set the temperature for text generation. Lower values (e.g., 0.0001) make output more deterministic, following likely predictions.\n",
        "# Higher values make the output more random.\n",
        "generation_config.temperature = 0.0001\n",
        "\n",
        "# Set the top-p sampling value. A value of 0.95 means focusing on the most likely words that make up 95% of the probability distribution.\n",
        "generation_config.top_p = 0.95\n",
        "\n",
        "# Enable text sampling. When set to True, the model randomly selects words based on their probabilities, introducing randomness.\n",
        "generation_config.do_sample = True\n",
        "\n",
        "# Set the repetition penalty. A value of 1.15 discourages the model from repeating the same words or phrases too frequently in the output.\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "\n",
        "# Create a text generation pipeline using the initialized model, tokenizer, and generation configuration\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Create a LangChain pipeline that wraps the text generation pipeline and set a specific temperature for generation\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As a machine learning expert, I would recommend including the following topics in your CV:\n",
            "1. Academic Qualifications: List all your relevant academic qualifications, including degrees, institutions, and graduation dates. Highlight any notable achievements or awards received during your studies.\n",
            "2. Research Experience: Include any research experiences you have had, such as research assistant positions, internships, or writing projects. Provide detailed information about each project, including the topic, methodology, and outcomes.\n",
            "3. Professional Experience: List any professional experience you have gained, such as internships, freelance work, or full-time jobs. Highlight any relevant skills or software proficiencies you possess, such as programming languages like Python or SQL, data analysis tools like Excel or Tableau, or machine learning frameworks like TensorFlow or PyTorch.\n",
            "4. Research Interests: Outline your current research interests and areas of focus. This can include specific topics, industries, or technologies you are passionate about, as well as any ongoing research projects or collaborations.\n",
            "5. Teaching Experience: If you have any teaching experience, include details about the courses you have taught, the institutions where you taught, and any relevant certifications or training.\n",
            "6. Skills: List any relevant skills you possess, such as programming languages, data analysis tools, or machine learning frameworks. Also, mention any soft skills like communication, teamwork, or problem-solving abilities.\n",
            "7. Awards and Honors: Include any notable awards or honors you have received, such as best paper awards, scholarships, or grants.\n",
            "8. Publication History: List any publications you have authored, including journal articles, conference proceedings, or book chapters. Provide details about the publication, such as title, authors, publication date, and citation counts.\n",
            "9. Presentations: Include any presentations you have given, including conference talks, seminar presentations, or workshop sessions. Provide details about the presentation, such as title, location, date, and audience.\n",
            "10. Professional Memberships: List any professional memberships or associations you are part of, including industry groups, academic societies, or open-source communities.\n",
            "By including these topics in your CV, you will demonstrate your expertise and commitment to the field of machine learning, making it easier for potential employers or collaborators to evaluate your qualifications and identify areas of common interest.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain import PromptTemplate\n",
        " \n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Act as a ML expert. Use the following information to answer the question at the end.\n",
        "<</SYS>>\n",
        " \n",
        "{context}\n",
        " \n",
        "{question} [/INST]\n",
        "\"\"\"\n",
        " \n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        " \n",
        " \n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        " \n",
        "result = qa_chain(\n",
        "    \"What topics should I include in my CV?\"\n",
        ")\n",
        "print(result[\"result\"].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 3 most important topics of a CV are: 1. Personal details - This section\n",
            "includes your name, address, phone number, professional email, and visa status\n",
            "(if relevant). It is essential to provide accurate and up-to-date contact\n",
            "information to make it easy for potential employers to reach out to you. 2.\n",
            "Career summary - This section should be around 5-7 sentences summarizing your\n",
            "expertise in your disciplines, years of experience, notable research findings,\n",
            "key achievements, and publications. It provides an overview of your career path\n",
            "and helps recruiters quickly determine if you fit the job requirements. 3.\n",
            "Education - In this section, provide an overview of your education history\n",
            "starting from your most recent academic degree obtained (in reverse\n",
            "chronological order). Highlight any relevant courses, certifications, or\n",
            "training programs that align with the job requirements.\n"
          ]
        }
      ],
      "source": [
        "from textwrap import fill\n",
        " \n",
        "result = qa_chain(\n",
        "    \"Summarize the 3 most important topics of the CV.\"\n",
        ")\n",
        "print(fill(result[\"result\"].strip(), width=80))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
